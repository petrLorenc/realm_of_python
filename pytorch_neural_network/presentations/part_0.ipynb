{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Disclaimer\n",
    "\n",
    "This workshop is heavily inspired by Andrej Karpathy's youtube video series on neural networks - mainly https://www.youtube.com/watch?v=VMj-3S1tku0\n",
    "\n",
    "My approach was that I have watched it several times and then tried to reproduce it from scratch (memory) because it was very educational. Now I wanted to give you a sneak peek into what I have learned/\"refreshed from memory\".\n",
    "\n",
    "## Takeaways\n",
    "   * You should have intuition about why the code below works and why it has this \"interface\".\n",
    "   * You should have intuition about what gradient is and how it is used in the context of neural networks.\n",
    "   * Be motivated to learn more :)"
   ],
   "id": "32879edcc9b2a1ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:13:22.034732Z",
     "start_time": "2025-07-30T08:13:13.959865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "train_y = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.random.manual_seed(42)  # Set a seed for reproducibility\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid() # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x) # x = torch.relu(x) # much faster convergence\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "# Mean Squared Error (MSE) - the average of the squares of the errors, that is, the average squared difference between the estimated values and the actual value\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "# Stochastic - it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data)\n",
    "# https://en.wikipedia.org/wiki/Gradient_descent (see Gradient Descent in 2D)\n",
    "# Gradient - function whose value at a point gives the direction and the rate of fastest increase\n",
    "# Gradient Descent - adding minus to go in the direction of the steepest descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# randomly initialized weights\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))\n",
    "\n",
    "\"\"\"\n",
    "train_y = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]\n",
    "\"\"\""
   ],
   "id": "ea542686b39b60a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6653], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6683], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6511], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6557], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:13:43.792732Z",
     "start_time": "2025-07-30T08:13:43.783922Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "7defa31fa4691b8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:13:56.409651Z",
     "start_time": "2025-07-30T08:13:56.402462Z"
    }
   },
   "cell_type": "code",
   "source": "model.fc1.weight",
   "id": "c09cbc80f823f0ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5406,  0.5869],\n",
       "        [-0.1657,  0.6496]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:14:07.001482Z",
     "start_time": "2025-07-30T08:14:06.997891Z"
    }
   },
   "cell_type": "code",
   "source": "model.fc1.bias",
   "id": "478a878f6fe7fd6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1549,  0.1427], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:14:43.236779Z",
     "start_time": "2025-07-30T08:14:43.224462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_input = torch.tensor(train_x[0], dtype=torch.float32)\n",
    "model(_input)"
   ],
   "id": "c046ceb5e3cebee2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6653], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:14:51.407652Z",
     "start_time": "2025-07-30T08:14:51.400713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_input = torch.tensor(train_x[0], dtype=torch.float32)\n",
    "_output = ((_input @ model.fc1.weight.t() + model.fc1.bias).sigmoid() @ model.fc2.weight.t() + model.fc2.bias).sigmoid()\n",
    "_output"
   ],
   "id": "f8652d9077cebebf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6653], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:17:39.223581Z",
     "start_time": "2025-07-30T08:17:33.711450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stochastic Gradient Descent\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # batch training - size 1\n",
    "    for i in range(len(train_x)):\n",
    "        inputs = torch.tensor(train_x[i], dtype=torch.float32)\n",
    "        target = torch.tensor(train_y[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad() # we will see why this is important later\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward() # we will focus on this part specifically\n",
    "        optimizer.step()"
   ],
   "id": "245cbee578ce99ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36e40cd0c75342b39befd084e599fc6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:17:42.464363Z",
     "start_time": "2025-07-30T08:17:42.459391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "id": "44c570dbc2529c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0483], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9567], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9425], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0434], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:18:28.554035Z",
     "start_time": "2025-07-30T08:18:27.364070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full Gradient Descent\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.)\n",
    "\n",
    "inputs = torch.tensor(train_x, dtype=torch.float32)\n",
    "targets = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))\n",
    "\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # full batch - usually not feasible for large datasets\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward() # we will focus on this part specifically\n",
    "    optimizer.step()\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "id": "d90b755663954e3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5916], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6037], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5989], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6101], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ea5bd693ba04663b104c09522f48de4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0157], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.4996], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9828], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5006], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:21:12.368282Z",
     "start_time": "2025-07-30T08:21:11.174896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full Gradient Descent without optimizer and loss\n",
    "model = Model()\n",
    "inputs = torch.tensor(train_x, dtype=torch.float32)\n",
    "targets = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))\n",
    "\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # Gradient descent - not stochastic\n",
    "    for p in model.parameters():\n",
    "        p.grad = None  # reset gradients\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = (outputs - targets).pow(2).mean()  # Mean Squared Error (MSE) loss\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= 1. * p.grad  # update weights manually"
   ],
   "id": "c6171320e8b092be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3454], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.3412], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.3613], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.3574], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8984c785048475ca2fac71a369374ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:21:15.698258Z",
     "start_time": "2025-07-30T08:21:15.692605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "id": "eb8724cef5fec15d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0171], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9819], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9819], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0223], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# the predictions is just a series of mathematical operations (which should be differentiable)",
   "id": "97c0569acbaa254c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Where the ChatGPT (from the title of the presentation) is?\n",
    "\n",
    "It is \"hidden\" in another Karpathy's repository - https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "It is just Pytorch module (same as above) but with more layers and more complex architecture.\n",
    "\n",
    "So if you understand the code above, you are step closer to understanding the ChatGPT architecture (more specifically GPT2, but it is not such clickbait :) )."
   ],
   "id": "9e315cdf4fd0dabb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46a7304f7258dc0d"
  },
  {
   "cell_type": "code",
   "id": "82327bb3938f2545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:57:49.754204Z",
     "start_time": "2025-07-29T08:57:36.952878Z"
    }
   },
   "source": [
    "train_x = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "train_y = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.random.manual_seed(42)  # Set a seed for reproducibility\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid() # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x) # x = torch.relu(x) # much faster convergence\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "# Mean Squared Error (MSE) - the average of the squares of the errors, that is, the average squared difference between the estimated values and the actual value\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "# Stochastic - it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data)\n",
    "# https://en.wikipedia.org/wiki/Gradient_descent (see Gradient Descent in 2D)\n",
    "# Gradient - function whose value at a point gives the direction and the rate of fastest increase\n",
    "# Gradient Descent - adding minus to go in the direction of the steepest descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# randomly initialized weights\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6653], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6683], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6511], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.6557], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "995c0d87-e3f6-4d56-aafa-018e2112c340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:58:00.793122Z",
     "start_time": "2025-07-29T08:58:00.789769Z"
    }
   },
   "source": [
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:58:20.613083Z",
     "start_time": "2025-07-29T08:58:20.609776Z"
    }
   },
   "cell_type": "code",
   "source": "model.fc1.weight",
   "id": "aa07b83796cc2309",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5406,  0.5869],\n",
       "        [-0.1657,  0.6496]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:58:28.621983Z",
     "start_time": "2025-07-29T08:58:28.618286Z"
    }
   },
   "cell_type": "code",
   "source": "model.fc1.bias",
   "id": "2b3d0d5c6655c9b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1549,  0.1427], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:00:23.996281Z",
     "start_time": "2025-07-29T09:00:23.992509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_input = torch.tensor(train_x[0], dtype=torch.float32)\n",
    "_output = ((_input @ model.fc1.weight.t() + model.fc1.bias).sigmoid() @ model.fc2.weight.t() + model.fc2.bias).sigmoid()\n",
    "_output"
   ],
   "id": "be927753b9524668",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6653], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "235378ad49305880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:01:04.348082Z",
     "start_time": "2025-07-29T09:00:59.145601Z"
    }
   },
   "source": [
    "# Stochastic Gradient Descent\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # batch training - size 1\n",
    "    for i in range(len(train_x)):\n",
    "        inputs = torch.tensor(train_x[i], dtype=torch.float32)\n",
    "        target = torch.tensor(train_y[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad() # we will see why this is important later\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward() # we will focus on this part specifically\n",
    "        optimizer.step()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18e4df57e64c47fca63adb5ab340293c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "18ae4202983f31d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:01:05.269089Z",
     "start_time": "2025-07-29T09:01:05.263Z"
    }
   },
   "source": [
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0483], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9567], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9425], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0434], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:09:00.543795Z",
     "start_time": "2025-07-29T09:08:59.425275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full Gradient Descent\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.)\n",
    "\n",
    "inputs = torch.tensor(train_x, dtype=torch.float32)\n",
    "targets = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))\n",
    "\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # full batch - usually not feasible for large datasets\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward() # we will focus on this part specifically\n",
    "    optimizer.step()\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "id": "2ecabc2f24562c79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5826], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5756], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5895], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5811], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "345878e34f7641af82bbf80b068d4f60"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0169], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9840], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9808], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0150], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "11118502483e946d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:11:12.734886Z",
     "start_time": "2025-07-29T09:11:11.497564Z"
    }
   },
   "source": [
    "# Full Gradient Descent without optimizer and loss\n",
    "model = Model()\n",
    "inputs = torch.tensor(train_x, dtype=torch.float32)\n",
    "targets = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))\n",
    "\n",
    "for epoch in tqdm(range(10_000)):\n",
    "    # Gradient descent - not stochastic\n",
    "    for p in model.parameters():\n",
    "        p.grad = None  # reset gradients\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = (outputs - targets).pow(2).mean()  # Mean Squared Error (MSE) loss\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= 1. * p.grad  # update weights manually"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5726], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5547], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5736], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5557], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5164cc7a09ee40af89734149a2781aaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f3f6487e66f399ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:11:13.404726Z",
     "start_time": "2025-07-29T09:11:13.397999Z"
    }
   },
   "source": [
    "# after training\n",
    "print(model(torch.tensor(train_x[0], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[1], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[2], dtype=torch.float32)))\n",
    "print(model(torch.tensor(train_x[3], dtype=torch.float32)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0210], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9820], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9820], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0182], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5637037bd03f0fd",
   "metadata": {},
   "outputs": [],
   "source": "# the predictions is just a series of mathematical operations (which should be differentiable)"
  },
  {
   "cell_type": "markdown",
   "id": "a161c522ab950f49",
   "metadata": {},
   "source": [
    "Where the ChatGPT (from the title of the presentation) is?\n",
    "\n",
    "It is \"hidden\" in another Karpathy's repository - https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "It is just Pytorch module (same as above) but with more layers and more complex architecture.\n",
    "\n",
    "So if you understand the code above, you are step closer to understanding the ChatGPT architecture (more specifically GPT2, but it is not such clickbait :) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924193e50a4dd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
